输入图片从原始数据（.jpg）转换为 PyTorch Tensor（张量）。

具体转换过程：

```
原始JPG文件 → PIL.Image对象 → PyTorch Tensor
              ↓
        形状: (H, W, 3) → (3, H, W)
        数值: 0-255整数 → 0.0-1.0浮点数
```

代码实现：

```python
# dataset.py中的转换
transform = transforms.Compose([
    transforms.Resize((224, 224)),           # PIL.Image调整大小
    transforms.ToTensor(),                   # PIL.Image → Tensor
    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # 标准化
                         std=[0.229, 0.224, 0.225])
])

# 最终Tensor结构：
# shape: [3, 224, 224]  # [通道数, 高度, 宽度]
# dtype: torch.float32
# 数值范围: ~[-2.0, 2.0] (经过标准化后)
```



transforms.Compose 是 转换组合器。

功能：把多个数据预处理步骤串联成一个流水线。

类比：像工厂的装配线：

```
原材料 → 步骤1 → 步骤2 → 步骤3 → 成品
```

代码示例：

```python
transform = transforms.Compose([
    transforms.Resize(224),      # 第1步：调整大小
    transforms.ToTensor(),       # 第2步：转成张量  
    transforms.Normalize(...)    # 第3步：标准化
])

# 使用：一张图片按顺序经过这三个步骤
processed_image = transform(原始图片)
```

作用：让数据预处理代码更简洁、可复用。



Normalize() 是标准化（归一化）操作。

作用：把图像像素值从 [0, 1] 范围缩放到均值为0、标准差为1的标准正态分布附近。

公式：

```
output = (input - mean) / std
```

你的代码中：

```python
Normalize(mean=[0.485, 0.456, 0.406],  # RGB三通道的均值
          std=[0.229, 0.224, 0.225])   # RGB三通道的标准差
```

效果：输入图像更符合ImageNet数据集统计特性，有利于模型收敛。


conv1 是第一个卷积层。

结构：

· 卷积核大小：7×7
· 输入通道：3（RGB）
· 输出通道：64
· 步长（stride）：2
· 填充（padding）：3

工作原理：

```
7×7的窗口在224×224图像上滑动
每个位置：计算3×7×7=147个值的加权和
输出：64个特征图（112×112尺寸）
```

数学表达：

```
每个输出特征图[i,j] = ∑∑∑ (输入区域 × 卷积核权重) + 偏置
```

作用：提取低级视觉特征（边缘、纹理、颜色）。


三个问题的答案：

1. 64个特征图怎么确定的？

经验设计：ResNet论文作者通过实验确定的超参数。第一层通常用64或32通道，64提供更多特征表达能力。

2. 112×112怎么确定的？

计算公式：

```
输出尺寸 = (输入尺寸 - 卷积核大小 + 2×填充) ÷ 步长 + 1
224 - 7 + 2×3 = 224 - 7 + 6 = 223
223 ÷ 2 = 111.5 → 向下取整 = 111 ❌
```

实际正确计算：

```
(224 - 7 + 2×3) ÷ 2 + 1 
= (224 - 7 + 6) ÷ 2 + 1
= 223 ÷ 2 + 1
= 111.5 + 1
= 112.5 → PyTorch中向下取整 = 112
```

3. 为什么边长除以二？（步长=2）

目的：降低特征图尺寸，减少计算量，增加感受野。

效果：

· 计算量减少约75%
· 每个像素感受野增大
· 为后续下采样做准备


BatchNorm（批归一化）作用原理：

1. 计算过程（对每个批次）：

```
输入：一个批次的特征图 [N, C, H, W]
1. 计算批次均值：μ = mean(输入)
2. 计算批次方差：σ² = var(输入)  
3. 归一化：x̂ = (x - μ) / √(σ² + ε)
4. 缩放平移：y = γ × x̂ + β
```

2. 参数：

· γ（权重）：可学习的缩放参数
· β（偏置）：可学习的平移参数
· ε：防止除零的小常数（1e-5）

3. 作用：

· 稳定训练：防止梯度消失/爆炸
· 加速收敛：允许使用更大学习率
· 轻微正则化：减少对Dropout依赖

4. 在ResNet中：

每个卷积层后都接BatchNorm，保证特征分布稳定。


ReLU（Rectified Linear Unit）激活函数

数学公式：

```
f(x) = max(0, x)
```

工作原理：

```
输入 x → 如果 x > 0 → 输出 x
        如果 x ≤ 0 → 输出 0
```

特点：

1. 稀疏激活：负值全为0，只有部分神经元激活
2. 缓解梯度消失：正区间梯度恒为1
3. 计算简单：只有比较和赋值操作

在ResNet中作用：

· 引入非线性，使网络能学习复杂模式
· 残差连接后使用：output = ReLU(f(x) + shortcut)

因为 ReLU不是线性函数。

数学证明：

1. 线性函数的定义：

```
f(x) 是线性的当且仅当：
1. f(a + b) = f(a) + f(b)  （可加性）
2. f(k × x) = k × f(x)     （齐次性）
```

2. 测试ReLU是否线性：

取 a = 2, b = -3, k = 2

```
ReLU(2 + (-3)) = ReLU(-1) = 0
ReLU(2) + ReLU(-3) = 2 + 0 = 2
0 ≠ 2  ❌ 不满足可加性

ReLU(2 × (-1)) = ReLU(-2) = 0  
2 × ReLU(-1) = 2 × 0 = 0
0 = 0  ✓ 满足齐次性

结论：不满足线性函数定义
```

3. 可视化理解：

```
线性函数：一条直线 y = kx + b
ReLU函数：两条线段组成的折线
          ↗
          /
         /
________/  (原点处有拐点)
```

拐点（x=0）处的非线性使得神经网络能够学习复杂模式。



MaxPool（最大池化）工作原理：

操作过程：

```
在特征图上滑动一个窗口（如3×3）
取窗口内的最大值作为输出
移动步长（stride）通常为2
```

示例（2×2 MaxPool，stride=2）：

```
输入 4×4：    输出 2×2：
[1, 3, 2, 4]    [3, 4]
[5, 7, 6, 8] →  [7, 8]  
[9,1, 3, 5]     [9, 5]
[2, 4, 6, 8]    [4, 8]
```

在ResNet中的参数：

```python
nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
# 窗口大小：3×3
# 步长：2（尺寸减半）
# 填充：1（保持边界信息）
```

作用：

1. 降维：减少特征图尺寸（56×56 → 28×28）
2. 平移不变性：小幅位移不影响输出
3. 保留显著特征：只传递最强的激活值

详细解释下采样模块

作用：

调整捷径连接的维度，使其能与主路径输出相加。

代码逐行解释：

1. 初始化

```python
downsample = None
```

先设为None，表示默认不需要下采样。

2. 判断条件

```python
if stride != 1 or self.in_channels != out_channels:
```

两个触发条件（满足任一就需要下采样）：

条件A：stride != 1

· 当第一个残差块的步长>1时，特征图尺寸会缩小
· 例：输入56×56，stride=2 → 输出28×28
· 问题：捷径连接的输入是56×56，无法与28×28的主路径输出相加
· 解决：通过1×1卷积将捷径连接也下采样到28×28

条件B：self.in_channels != out_channels

· 当输入输出通道数不同时
· 例：Layer2中，输入64通道，输出128通道
· 问题：64通道的捷径连接无法与128通道的主路径输出相加
· 解决：通过1×1卷积将64通道扩展为128通道

3. 下采样模块结构

```python
downsample = nn.Sequential(
    nn.Conv2d(self.in_channels, out_channels, kernel_size=1, 
              stride=stride, bias=False),
    nn.BatchNorm2d(out_channels)
)
```

3.1 1×1卷积层

```python
nn.Conv2d(self.in_channels, out_channels, kernel_size=1, 
          stride=stride, bias=False)
```

· kernel_size=1：只进行通道变换，不改变空间特征
· stride=stride：与主路径同步下采样（如果stride>1）
· 作用：调整通道数和特征图尺寸

3.2 批归一化层

```python
nn.BatchNorm2d(out_channels)
```

· 保持与主路径一致的归一化处理
· 保证捷径连接输出的分布稳定

实际示例：

例1：Layer2的第一块

```python
# 输入: [batch, 64, 56, 56]
# 输出: [batch, 128, 28, 28]
# 条件: stride=2 (尺寸减半), 64≠128 (通道不同)
# 下采样模块执行：
# 1. 1×1卷积: 64通道→128通道, stride=2
# 2. 输出: [batch, 128, 28, 28]
```

例2：Layer1的第一块

```python
# 输入: [batch, 64, 56, 56]  
# 输出: [batch, 64, 56, 56]
# 条件: stride=1 (尺寸不变), 64=64 (通道相同)
# 下采样模块: None (直接恒等映射)
```

数学表达：

```
设捷径连接输入为x
主路径输出为F(x)

# 无下采样时：
输出 = ReLU(F(x) + x)

# 有下采样时：
输出 = ReLU(F(x) + W_s × x)  # W_s是1×1卷积权重
```

设计意义：

1. 维度匹配：确保残差相加可行
2. 计算高效：1×1卷积参数量少
3. 信息保留：捷径连接仍传递原始信号



nn.Sequential 是顺序容器。

功能：

按顺序执行多个网络层。

类比：

像管道的连接：

```
输入 → 层1 → 层2 → 层3 → 输出
```

代码示例：

```python
# 创建
downsample = nn.Sequential(
    nn.Conv2d(64, 128, kernel_size=1, stride=2),
    nn.BatchNorm2d(128)
)

# 使用（等价于手动调用）
output = downsample(x)
# 等价于：
# conv_out = conv(x)
# output = bn(conv_out)
```

在下采样中的作用：

把1×1卷积和批归一化打包成一个模块，简化代码。


全局平均池化（Global Average Pooling）

操作：

对每个通道的整个特征图取平均值。

示例：

```
输入特征图：512个通道，每个7×7
[ [1.2, 0.8, ...],  # 通道1的7×7矩阵
  [0.5, 1.3, ...],  # 通道2的7×7矩阵
  ...               # 共512个通道
]

全局平均池化后：
对每个7×7矩阵求平均值 → 512个标量值
输出：[0.9, 1.1, ..., 0.7]  # 长度512的向量
```

在ResNet中的代码：

```python
self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
# 输入: [batch, 512, 7, 7]
# 输出: [batch, 512, 1, 1]
```

作用：

1. 降维：将7×7的特征图变成1×1
2. 减少参数：替代全连接层，防止过拟合
3. 空间信息汇总：每个通道保留一个最具代表性的值


全连接层（Fully Connected Layer）

结构：

每个输入神经元连接到每个输出神经元。

数学表达：

```
输出 = 输入 × 权重矩阵 + 偏置
y = Wx + b
```

在ResNet中的代码：

```python
self.fc = nn.Linear(512, num_classes)
# 输入: 512维向量
# 输出: 2维向量（患病/健康概率）
```

示例计算：

```
输入: [0.9, 1.1, 0.7, ...]  # 512个值
权重: 512×2的矩阵
计算: 每个输出 = ∑(输入_i × 权重_ij) + 偏置_j
输出: [0.3, 0.7]  # 患病概率0.3，健康概率0.7
```

作用：

将提取的512个特征组合，输出分类概率。
